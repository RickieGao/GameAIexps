# 实验日志：
## 7月26日修改：
### 修改游戏：
* 取消延迟以提高训练速度
* 修改游戏reward为-1，0.1
* 提高敌机出现频率40%
### 修改超参数：
* REPLAY_MEMORY：50000 --> 100000
* OBSERVE = 100000 --> 150000
* EXPLORE = 2000000 --> 3000000
* FINAL_EPSILON = 0.0001 --> 0.1
* INITIAL_EPSILON = 1
* GAMMA = 0.99 --> 0.95
* ACTIONS = 3
* FRAME_PER_ACTION = 1
### 修改记录内容：
* 记录每局游戏的reward的同时，也记录单位时间内的总的reward
### 跑实验：
* 实验一：基本DQN
* 实验二：带有0.5的概率的rule
* 实验三：rule的概率从0.5减到0
* 服务器：112
## 7月27日：
### 修改超参数，增加一组超参数：
* REPLAY_MEMORY：150000
* OBSERVE = 150000
* EXPLORE = 3000000
* FINAL_EPSILON = 0.1
* INITIAL_EPSILON = 1
* GAMMA = 0.95 --> 0.90
* ACTIONS = 3
* FRAME_PER_ACTION = 1
* reward：-1， 0.1
### 跑实验：
* 实验五（Exp05）
* 服务器：130
## 8月1日：
### 修改游戏：
* 持续发子弹
* 动作空间减少到3个：不动、左移、右移
### 修改超参数：
* REPLAY_MEMORY：50000
* OBSERVE = 10000
* EXPLORE = 3000000
* FINAL_EPSILON = 0.0001
* INITIAL_EPSILON = 0.07
* GAMMA = 0.99
* ACTIONS = 3
* FRAME_PER_ACTION = 1
* reward：-1, 1, 0.1
### 跑实验：
* 实验6，原始DQN（Exp06）
* 实验7，0.5的概率的rule（Exp07）
* 服务器：112
## 8月2日：
### 超参数保持不变，增加一组实验：
* rule概率由1-epsilon递减为0
### 跑实验：
* 实验8
* 服务器：112

## 8月3日
### 减小explorer大小（加快epsilon和gamma的衰减速度）
* EXPLORE = 1000000
### 跑实验：
* 实验9：干预概率递减，从1到0
* 实验10：原始DQN
* 服务器：112

## 8月7日
### 新的rule：
* rule2：按照水平方向上的敌机的距离来打
* rule概率：递减
### 实验参数：
* REPLAY_MEMORY：50000
* OBSERVE = 10000
* EXPLORE = 1000000
* FINAL_EPSILON = 0.0001
* INITIAL_EPSILON = 0.07
* GAMMA = 0.99
* ACTIONS = 3
* FRAME_PER_ACTION = 1
* reward：-1, 1, 0.1
### 跑实验:
* 实验11
* 服务器:112

## 8月10日
### 修改rule生效的概率衰减模型，不采用原先的线性衰减：
* 实验12：rule1 指数衰减，衰减率为0.9：omega = INITIAL_OMEGA * (DECAY_RATE ** (t / DECAY_STEPS))
* 实验13：rule2 指数衰减，衰减率为0.8：omega = INITIAL_OMEGA * (DECAY_RATE ** (t / DECAY_STEPS))
### 跑实验：
* 实验12、 13
* 服务器：112

## 8月14日
### 重新跑实验10，增加数据量：
* 实验10
* 服务器：112

## 8月16日：
### 修改：
* 图像处理函数
* 相应的动作规则函数

## 8月29日：
### 修改：
* 屏幕变短。飞机位置下移。
### 跑实验：
* 实验15：在实验10的基础上修改的
* 实验16：在实验13的基础上修改的
* 服务器：130

## 11月18日：
### 修改：
* 增加随机种子，保持游戏setting完全一致
### 跑实验：
* 实验19：反向rule指数递减（在实验13的基础上修改）
* 实验20：可解释观测实验，不训练，增加观测值（在实验10的基础上修改）
* 服务器：112

##12月12日：
### 修改：
* 修改rule action文件，增加图片转化的流程
### 跑实验：
* 实验21：在实验13的基础上，根据最新的规则来改的
* 服务器：112

## 2020年1月7日：
### 重新跑3个实验，更正图像处理函数中的错误：
* 实验23：在实验10的基础上改的
* 实验24：在实验13的基础上改的，用rule2
* 实验25：在实验19的基础上改的，用反向的rule2

以后的实验数据都以这3个为准。